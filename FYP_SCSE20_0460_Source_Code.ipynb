{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FYP SCSE20-0460 Source Code",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwfw-Z025pLV"
      },
      "source": [
        "Mounting drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjudjAEatXMJ"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkrPgZuXp07r"
      },
      "source": [
        "Libaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S50Q_3Ovpyy0"
      },
      "source": [
        "import os, sys, json, random\r\n",
        "import numpy as np\r\n",
        "from numpy import dstack\r\n",
        "import time\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "import statistics\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.utils import shuffle\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "from sklearn import metrics\r\n",
        "\r\n",
        "from keras.regularizers import l2\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import LSTM\r\n",
        "from keras.layers import Dropout\r\n",
        "\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn.tree import DecisionTreeClassifier\r\n",
        "from sklearn.naive_bayes import GaussianNB\r\n",
        "from sklearn.ensemble import AdaBoostClassifier\r\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX8KT6Ep0L8M"
      },
      "source": [
        "Randomness seed for reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjmPudgI0OFB"
      },
      "source": [
        "seed = 1\r\n",
        "random.seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ud7rMsQGp2Kb"
      },
      "source": [
        "Reading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dhza5OyUl8g_"
      },
      "source": [
        "benign_data    = {}\r\n",
        "malware_data   = {}\r\n",
        "report = {}\r\n",
        "\r\n",
        "benign_dict = {}\r\n",
        "malware_dict   = {}\r\n",
        "\r\n",
        "benign_files = [\"/content/drive/MyDrive/FYP/open_source_data/amd_desktop_metadata/benign_post_pca_data.txt\",\r\n",
        "                \"/content/drive/MyDrive/FYP/open_source_data/amd_python_benign_metadata/python_benign_post_pca_data.txt\"\r\n",
        "                ]\r\n",
        "malware_files = [\"/content/drive/MyDrive/FYP/open_source_data/amd_malware_metadata/malware_post_pca_data.txt\"]\r\n",
        "\r\n",
        "print(\"Files loaded properly\")\r\n",
        "#--------------------------------------------------------------------------------------------------------------------------------#\r\n",
        "\r\n",
        "# Benign data\r\n",
        "for files in benign_files:\r\n",
        "\r\n",
        "  with open(files, 'r') as data_file:\r\n",
        "    json_data = json.load(data_file)\r\n",
        "    print(json_data.keys())\r\n",
        "\r\n",
        "  for bm in json_data.keys():\r\n",
        "    if bm not in benign_dict:\r\n",
        "      benign_dict[bm]        = False\r\n",
        "      benign_data[bm]        = []\r\n",
        "    \r\n",
        "    for each_sample in json_data[bm]:\r\n",
        "      if benign_data[bm] == []:\r\n",
        "        benign_data[bm] = np.array(each_sample) \r\n",
        "      else:\r\n",
        "        benign_data[bm] = np.vstack((benign_data[bm], each_sample))\r\n",
        "  \r\n",
        "  data_file.close()\r\n",
        "\r\n",
        "print(\"Benign data read\")\r\n",
        "#--------------------------------------------------------------------------------------------------------------------------------#\r\n",
        "\r\n",
        "# Malware data\r\n",
        "for files in malware_files:\r\n",
        "\r\n",
        "  with open(files, 'r') as data_file:\r\n",
        "    json_data = json.load(data_file)\r\n",
        "    print(json_data.keys())\r\n",
        "\r\n",
        "  for bm in json_data.keys():\r\n",
        "    if bm not in malware_dict:\r\n",
        "      malware_dict[bm]        = False\r\n",
        "      malware_data[bm]        = []\r\n",
        "    \r\n",
        "    for each_sample in json_data[bm]:\r\n",
        "      if malware_data[bm] == []:\r\n",
        "        malware_data[bm] = np.array(each_sample) \r\n",
        "      else:\r\n",
        "        malware_data[bm] = np.vstack((malware_data[bm], each_sample))\r\n",
        "  \r\n",
        "  data_file.close()\r\n",
        "\r\n",
        "print(\"Malware data read\")\r\n",
        "#--------------------------------------------------------------------------------------------------------------------------------#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Od2MR8KzwHKz"
      },
      "source": [
        "print(list(benign_data.items())[2][0])\r\n",
        "print(len(benign_data))\r\n",
        "print(len(benign_data['notepad2']))\r\n",
        "print(len(benign_data['notepad2'][0]))\r\n",
        "print(benign_data['notepad2'][0])\r\n",
        "print(benign_data['pickledict'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-kF1Yvoxamd"
      },
      "source": [
        "print(list(malware_data.items())[2][0])\r\n",
        "print(len(malware_data))\r\n",
        "print(len(malware_data['813e882a972defdf261de319d1957c6a']))      #https://www.virustotal.com/gui/file/71b5fc8806f3b66b2b54e063d343b09393f25140fa5eb4498e31a7472a107c71/details\r\n",
        "print(len(malware_data['813e882a972defdf261de319d1957c6a'][0]))   \r\n",
        "print(malware_data['813e882a972defdf261de319d1957c6a'][0])\r\n",
        "print(malware_data['19a94ba578e83c4f11d13878e17c27d2'][0])        #https://www.virustotal.com/gui/file/48b38729f7bda05deb7c7e682b342f925c459eff8c9891252afad617dc2260f0/detection"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QW5_bUBx5Du"
      },
      "source": [
        "Sampling of data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bA9JAKjUx_aE"
      },
      "source": [
        "# Ensure equal number of benign keys and malware keys\r\n",
        "sample_size = min(len(benign_data.keys()), len(malware_data.keys()))\r\n",
        "\r\n",
        "print(sample_size)\r\n",
        "print(\"benign bm numbers \"  + str(len(benign_dict.keys())))\r\n",
        "print(\"malware bm numbers \" + str(len(malware_dict.keys())))\r\n",
        "print(\"Chosen Size \" + str(sample_size))\r\n",
        "\r\n",
        "print(\"Sample size chosen\")\r\n",
        "#--------------------------------------------------------------------------------------------------------------------------------#\r\n",
        "\r\n",
        "# Randomly choosing which benign keys are chosen\r\n",
        "selected_benign_samples = random.sample(benign_data.keys(), sample_size)\r\n",
        "for bm in list(benign_data.keys()):\r\n",
        "  if bm not in selected_benign_samples:\r\n",
        "    del benign_data[bm]\r\n",
        "\r\n",
        "print(\"Excess benign data removed\")\r\n",
        "#--------------------------------------------------------------------------------------------------------------------------------#\r\n",
        "\r\n",
        "# Just for show, since sample_size = malware_data.keys() size\r\n",
        "selected_malware_samples = random.sample(malware_data.keys(), sample_size)\r\n",
        "\r\n",
        "for bm in list(malware_data.keys()):\r\n",
        "  if bm not in selected_malware_samples:\r\n",
        "    del malware_data[bm]\r\n",
        "\r\n",
        "print(\"Excess malware data removed\")\r\n",
        "#--------------------------------------------------------------------------------------------------------------------------------#\r\n",
        "\r\n",
        "# Changing types to list\r\n",
        "for bm in benign_data:\r\n",
        "  benign_data[bm] = benign_data[bm].tolist()\r\n",
        "\r\n",
        "for bm in malware_data:\r\n",
        "  malware_data[bm] = malware_data[bm].tolist()\r\n",
        "\r\n",
        "print(\"Type change to list done\")\r\n",
        "#--------------------------------------------------------------------------------------------------------------------------------#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiLKFGX3N3cs"
      },
      "source": [
        "Dump of data chosen for experiment (optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBeqBmbFUxEJ"
      },
      "source": [
        "'''\r\n",
        "with open(\"/content/test1\", 'w') as outfile:\r\n",
        "  json.dump(benign_data, outfile, indent=2)\r\n",
        "outfile.close()\r\n",
        "\r\n",
        "with open(\"/content/test2\", 'w') as outfile:\r\n",
        "  json.dump(malware_data, outfile, indent=2)\r\n",
        "outfile.close()\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00DmJvYkN_0W"
      },
      "source": [
        "Creating relevant functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRgkKYAUOCtf"
      },
      "source": [
        "# For combining benign and malware data into appropriate X_train etc variables\r\n",
        "def split_and_stack(data, X_train, y_train, X_test, y_test, type='benign'):\r\n",
        "\r\n",
        "  #marking benign as 1, malware as 0\r\n",
        "  for bm in data:\r\n",
        "    if type == 'benign':\r\n",
        "      labels = np.ones(len(data[bm]))\r\n",
        "    else:\r\n",
        "      labels = np.zeros(len(data[bm]))\r\n",
        "            \r\n",
        "    single_X_train, single_X_test, single_y_train, single_y_test = train_test_split(data[bm], labels, test_size = 0.1, random_state = seed)\r\n",
        "\r\n",
        "    if len(X_train) == 0:\r\n",
        "        X_train = np.array(single_X_train)\r\n",
        "        y_train = np.array(single_y_train)\r\n",
        "        X_test  = np.array(single_X_test)\r\n",
        "        y_test  = np.array(single_y_test)\r\n",
        "\r\n",
        "    else:\r\n",
        "        X_train = np.vstack((X_train, single_X_train))\r\n",
        "        y_train = np.concatenate((y_train, single_y_train))\r\n",
        "        X_test  = np.vstack((X_test, single_X_test))\r\n",
        "        y_test  = np.concatenate((y_test, single_y_test))\r\n",
        "\r\n",
        "  print(\"split_and_stack success\")\r\n",
        "\r\n",
        "  return X_train, y_train, X_test, y_test\r\n",
        "\r\n",
        "# For LSTM model creation\r\n",
        "def create_LSTM_model():\r\n",
        "  model = Sequential()\r\n",
        "\r\n",
        "  model.add(LSTM(num_neurons, dropout = 0.2, return_sequences = True, input_shape = (X_train.shape[1], 1)))\r\n",
        "  model.add(LSTM(num_neurons, dropout = 0.2, return_sequences = True))\r\n",
        "  model.add(LSTM(num_neurons, dropout = 0.2))\r\n",
        "\r\n",
        "  model.add(Dense(1, activation = 'sigmoid'))\r\n",
        "\r\n",
        "  print(\"create_LSTM_model success\")\r\n",
        "\r\n",
        "  return model\r\n",
        "\r\n",
        "# For MLP (multi-layer perceptron) creation\r\n",
        "def create_MLP_model():\r\n",
        "  model = Sequential()\r\n",
        "\r\n",
        "  model.add(Dense(num_neurons))\r\n",
        "  model.add(Dropout(0.2))\r\n",
        "  \r\n",
        "  model.add(Dense(num_neurons))\r\n",
        "  model.add(Dropout(0.2))\r\n",
        "\r\n",
        "  model.add(Dense(num_neurons))\r\n",
        "  model.add(Dropout(0.2))\r\n",
        "\r\n",
        "  model.add(Dense(1, activation = 'sigmoid'))\r\n",
        "\r\n",
        "  print(\"create_MLP_model success\")\r\n",
        "\r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qj-f652lrFKw"
      },
      "source": [
        "Creating classifier functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeI0g_kjrI78"
      },
      "source": [
        "# Random Forest Classifier\r\n",
        "def create_RF_model():\r\n",
        "\r\n",
        "  # Variables\r\n",
        "  max_depth = 40      # for parameter_i in list(xrange(30, 40, 1))]\r\n",
        "  min_samples_split = 12\r\n",
        "  min_samples_leaf = 12\r\n",
        "  n_estimators = 100\r\n",
        "  max_features = None\r\n",
        "  random_state = seed\r\n",
        "\r\n",
        "  model = RandomForestClassifier(max_depth = max_depth, min_samples_split = min_samples_split,\r\n",
        "                                 min_samples_leaf = min_samples_leaf, n_estimators = n_estimators,\r\n",
        "                                 max_features = max_features, random_state = random_state)\r\n",
        "  \r\n",
        "  return model\r\n",
        "\r\n",
        "# Decision Tree Classifier\r\n",
        "def create_DT_model():\r\n",
        "\r\n",
        "  # Variables\r\n",
        "  max_depth = 40     # for parameter_i in list(xrange(35, 45, 1))]\r\n",
        "  min_samples_split = 12\r\n",
        "  min_samples_leaf = 12\r\n",
        "  presort = True\r\n",
        "  max_features = None\r\n",
        "  random_state = seed\r\n",
        "\r\n",
        "  model = DecisionTreeClassifier(max_depth = max_depth, min_samples_split = min_samples_split,\r\n",
        "                                 min_samples_leaf = min_samples_leaf, presort = True,\r\n",
        "                                 max_features = max_features, random_state = random_state)\r\n",
        "  \r\n",
        "  return model\r\n",
        "\r\n",
        "# Gaussian Naive Bayes\r\n",
        "def create_NB_model():\r\n",
        "\r\n",
        "  # Variables\r\n",
        "  priors = [0.5, 0.5]\r\n",
        "\r\n",
        "  model = GaussianNB(priors=priors)\r\n",
        "  \r\n",
        "  return model\r\n",
        "\r\n",
        "# AdaBoost Classifier\r\n",
        "def create_AB_model():\r\n",
        "\r\n",
        "  # Variables\r\n",
        "  algorithm = 'SAMME.R'   # is default\r\n",
        "  n_estimators = 500      # for parameter_i in list(xrange(300, 1300, 100))]\r\n",
        "  random_state = seed\r\n",
        "  \r\n",
        "  model = AdaBoostClassifier(algorithm = algorithm, n_estimators = n_estimators, random_state = random_state)\r\n",
        "  \r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvTYfoIGeN-R"
      },
      "source": [
        "Data preprocessing; Shuffling and splitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rz8ceqneQEO"
      },
      "source": [
        "X_train    = []\r\n",
        "X_test     = []\r\n",
        "y_train    = []\r\n",
        "y_test     = []\r\n",
        "\r\n",
        "# Combining benign and malware data\r\n",
        "X_train, y_train, X_test, y_test = split_and_stack(benign_data, X_train, y_train, X_test, y_test, type='benign')\r\n",
        "X_train, y_train, X_test, y_test = split_and_stack(malware_data, X_train, y_train, X_test, y_test, type='malware')\r\n",
        "\r\n",
        "# Test prints\r\n",
        "print(len(X_train))\r\n",
        "print(len(y_train))\r\n",
        "print(len(X_test))\r\n",
        "print(len(y_test))\r\n",
        "\r\n",
        "# Rejoining data to prepare for shuffling\r\n",
        "X_data  = np.vstack((X_train, X_test))\r\n",
        "y_data  = np.concatenate((y_train, y_test))\r\n",
        "\r\n",
        "# Test prints; confirm no lost data\r\n",
        "print(len(X_data))\r\n",
        "print(len(y_data))\r\n",
        "\r\n",
        "# Shuffling\r\n",
        "X_data, y_data = shuffle(X_data, y_data, random_state = seed)\r\n",
        "\r\n",
        "# Test print; reflects successful shuffle\r\n",
        "print(np.where(y_data==1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIO2sjdBQt1T"
      },
      "source": [
        "print(np.amax(X_data))\r\n",
        "print(np.amin(X_data))\r\n",
        "print(np.std(X_data))\r\n",
        "print(np.mean(X_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_DghVZMi1Ed"
      },
      "source": [
        "Data scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpvOUCYlcTAD"
      },
      "source": [
        "X_data = X_data / 100000000000000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKsjCRbWciFk"
      },
      "source": [
        "print(np.amax(X_data))\r\n",
        "print(np.amin(X_data))\r\n",
        "print(np.std(X_data))\r\n",
        "print(np.mean(X_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6gC2KVO0muZ"
      },
      "source": [
        "Training, val, test sets split;\r\n",
        "\r\n",
        "80-20 --> (60-20)-20"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXmZS5j53T_4"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=seed)   #80-20\r\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=seed)    #80 -> 60-20\r\n",
        "\r\n",
        "# Test print; confirm intended split\r\n",
        "print(len(X_train))\r\n",
        "print(len(y_train))\r\n",
        "print(len(X_val))\r\n",
        "print(len(y_val))\r\n",
        "print(len(X_test))\r\n",
        "print(len(y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Lj7BjMlCvki"
      },
      "source": [
        "Setting variables for 10-fold cross validation (k-fold)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A19CRVJvCywl"
      },
      "source": [
        "num_folds = 10\r\n",
        "X = X_train\r\n",
        "Y = y_train\r\n",
        "\r\n",
        "num_data = len(X_train)    \r\n",
        "nf = num_data//num_folds\r\n",
        "print(\"num_data = \", num_data)\r\n",
        "print(\"nf = \", nf)\r\n",
        "\r\n",
        "idx = np.arange(num_data)\r\n",
        "np.random.shuffle(idx)\r\n",
        "X, Y = X[idx], Y[idx]\r\n",
        "\r\n",
        "cross_val_accuracy = []\r\n",
        "mse_values = []\r\n",
        "time_per_fold = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2-FKEzlRP_x"
      },
      "source": [
        "Model training parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHx6fXpkROxL"
      },
      "source": [
        "num_neurons = 64\r\n",
        "epochs = 100\r\n",
        "batch_size = 32\r\n",
        "learning_rate = 0.001\r\n",
        "\r\n",
        "loss = tf.keras.losses.BinaryCrossentropy()\r\n",
        "#optimizer = 'SGD'\r\n",
        "optimizer = 'Adam'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1x9ElHXHDbf0"
      },
      "source": [
        "k-fold Cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QaYJiFKDc7R"
      },
      "source": [
        "\r\n",
        "for fold in range(num_folds):\r\n",
        "\r\n",
        "  # Manual splitting into train and test sets\r\n",
        "  start, end = fold*nf, (fold+1)*nf\r\n",
        "  X_eval_test, y_eval_test = X[start:end], Y[start:end]\r\n",
        "  X_eval_train  = np.append(X[:start], X[end:], axis=0)\r\n",
        "  y_eval_train = np.append(Y[:start], Y[end:], axis=0)\r\n",
        "\r\n",
        "  # Model creation\r\n",
        "  #model = create_LSTM_model()\r\n",
        "  model = create_MLP_model()\r\n",
        "\r\n",
        "  model.compile(optimizer=optimizer,\r\n",
        "                loss=loss,\r\n",
        "                metrics=['mse','accuracy'])\r\n",
        "  \r\n",
        "  # Reshape data to fit expected LSTM input shape\r\n",
        "  # For LSTM\r\n",
        "  #X_eval_train = np.reshape(X_eval_train, (X_eval_train.shape[0], X_eval_train.shape[1], 1))\r\n",
        "  #X_eval_test  = np.reshape(X_eval_test , (X_eval_test.shape[0] , X_eval_test.shape[1] , 1))\r\n",
        "  \r\n",
        "  # Start time recorded\r\n",
        "  start = time.time()\r\n",
        "  print(\"Time started\")\r\n",
        "\r\n",
        "  # Model training\r\n",
        "  history = model.fit(X_eval_train, y_eval_train,\r\n",
        "                      epochs=epochs,\r\n",
        "                      verbose=0,\r\n",
        "                      use_multiprocessing=False,\r\n",
        "                      validation_data=(X_eval_test, y_eval_test),\r\n",
        "                      batch_size=batch_size)     # [loss, accuracy, val_loss, val_accuracy]\r\n",
        "  \r\n",
        "  end = time.time()\r\n",
        "  print(\"Time ended\")\r\n",
        "\r\n",
        "  plt.plot(history.history['val_mse'], label='val_mse')\r\n",
        "  plt.xlabel('epochs')\r\n",
        "  plt.ylabel('val_mse')\r\n",
        "  plt.legend() \r\n",
        "\r\n",
        "  # summing the val_accuracy values across all 10 folds\r\n",
        "  cross_val_accuracy.append(history.history['val_accuracy'][-1])\r\n",
        "  mse_values.append(history.history['val_mse'][-1])\r\n",
        "  time_per_fold.append(end-start)\r\n",
        "  print(cross_val_accuracy)\r\n",
        "  print(mse_values)\r\n",
        "  print(time_per_fold)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDl3zUNkYyKN"
      },
      "source": [
        "Calculations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3n71NnTHYzKe"
      },
      "source": [
        "\r\n",
        "# Standard deviation\r\n",
        "print(np.std(cross_val_accuracy))\r\n",
        "\r\n",
        "# Mean\r\n",
        "print(np.mean(cross_val_accuracy))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOwGyrfs4EXt"
      },
      "source": [
        "Model fitting (Classifiers)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VueXG6o4FOE"
      },
      "source": [
        "'''\r\n",
        "# Creating model objects\r\n",
        "RF_model = create_RF_model()\r\n",
        "DT_model = create_DT_model()\r\n",
        "NB_model = create_NB_model()\r\n",
        "AB_model = create_AB_model()\r\n",
        "\r\n",
        "# Fitting to training data\r\n",
        "RF_model.fit(X_train, y_train)\r\n",
        "DT_model.fit(X_train, y_train)\r\n",
        "NB_model.fit(X_train, y_train)\r\n",
        "AB_model.fit(X_train, y_train)\r\n",
        "\r\n",
        "# Predicting on test data\r\n",
        "y_pred_RF = RF_model.predict(X_test)\r\n",
        "y_pred_DT = DT_model.predict(X_test)\r\n",
        "y_pred_NB = NB_model.predict(X_test)\r\n",
        "y_pred_AB = AB_model.predict(X_test)\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7rKDw1Ksk-H"
      },
      "source": [
        "Classifier results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNskJQYZbKcu"
      },
      "source": [
        "'''\r\n",
        "# Random Forest results\r\n",
        "print(\"RF results:\")\r\n",
        "print(\"RF Precision:\",metrics.precision_score(y_test, y_pred_RF))\r\n",
        "print(\"RF Recall:\",metrics.recall_score(y_test, y_pred_RF))\r\n",
        "print(\"RF F1:\",metrics.f1_score(y_test, y_pred_RF))\r\n",
        "\r\n",
        "# Decision Tree results\r\n",
        "print(\"DT results:\")\r\n",
        "print(\"DT Precision:\",metrics.precision_score(y_test, y_pred_DT))\r\n",
        "print(\"DT Recall:\",metrics.recall_score(y_test, y_pred_DT))\r\n",
        "print(\"DT F1:\",metrics.f1_score(y_test, y_pred_DT))\r\n",
        "\r\n",
        "# Gaussian NB results\r\n",
        "print(\"NB results:\")\r\n",
        "print(\"NB Precision:\",metrics.precision_score(y_test, y_pred_NB))\r\n",
        "print(\"NB Recall:\",metrics.recall_score(y_test, y_pred_NB))\r\n",
        "print(\"NB F1:\",metrics.f1_score(y_test, y_pred_NB))\r\n",
        "\r\n",
        "# AdaBoost results\r\n",
        "print(\"AB results:\")\r\n",
        "print(\"AB Precision:\",metrics.precision_score(y_test, y_pred_AB))\r\n",
        "print(\"AB Recall:\",metrics.recall_score(y_test, y_pred_AB))\r\n",
        "print(\"AB F1:\",metrics.f1_score(y_test, y_pred_AB))\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOP7cMUc4Zxa"
      },
      "source": [
        "Ensemble method (Classifiers, max voting)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyXZDUqy4Vq0"
      },
      "source": [
        "'''\r\n",
        "# Using predictions from RF, DT, AB\r\n",
        "# NB omitted due to poor precision\r\n",
        "# Max voting same result as averaging as there's only 3 sets - a max vote will have the same rounded average with binary values\r\n",
        "\r\n",
        "max_voting_pred = np.array([])\r\n",
        "\r\n",
        "for i in range(0, len(X_test)):\r\n",
        "    max_voting_pred = np.append(max_voting_pred, statistics.mode([y_pred_RF[i], y_pred_DT[i], y_pred_AB[i]]))\r\n",
        "\r\n",
        "print(\"Classifier Ensemble Max Voting Precision:\",metrics.precision_score(y_test, max_voting_pred))\r\n",
        "print(\"Classifier Ensemble Max Voting Recall:\",metrics.recall_score(y_test, max_voting_pred))\r\n",
        "print(\"Classifier Ensemble Max Voting F1:\",metrics.f1_score(y_test, max_voting_pred))\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKvE_lte8eUb"
      },
      "source": [
        "Ensemble method (Classifiers, blending)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDUHrVml5mEL"
      },
      "source": [
        "'''\r\n",
        "# Reshaping the predictions\r\n",
        "y_pred_RF = y_pred_RF.reshape(y_pred_RF.shape[0],1)\r\n",
        "y_pred_DT = y_pred_DT.reshape(y_pred_DT.shape[0],1)\r\n",
        "y_pred_AB = y_pred_AB.reshape(y_pred_AB.shape[0],1)\r\n",
        "\r\n",
        "# Test print\r\n",
        "print(y_pred_RF.shape)\r\n",
        "\r\n",
        "# Stacking the predictions\r\n",
        "X_stacked = y_pred_RF\r\n",
        "X_stacked = dstack((X_stacked, y_pred_DT))\r\n",
        "X_stacked = dstack((X_stacked, y_pred_DT))\r\n",
        "\r\n",
        "# Test print\r\n",
        "print(X_stacked.shape)\r\n",
        "\r\n",
        "# Reshaping stack\r\n",
        "X_stacked = X_stacked.reshape((X_stacked.shape[0], X_stacked.shape[1]*X_stacked.shape[2]))\r\n",
        "\r\n",
        "# Test print\r\n",
        "print(X_stacked.shape)\r\n",
        "\r\n",
        "\r\n",
        "# Logistic Regression model creation,fitting,predicting\r\n",
        "model = LogisticRegression()\r\n",
        "\r\n",
        "model.fit(X_stacked, y_test)\r\n",
        "\r\n",
        "y_stacked_test_pred = model.predict(X_stacked)\r\n",
        "\r\n",
        "print(\"Classifier Ensemble Blending Precision:\",metrics.precision_score(y_test, y_stacked_test_pred))\r\n",
        "print(\"Classifier Ensemble Blending Recall:\",metrics.recall_score(y_test, y_stacked_test_pred))\r\n",
        "print(\"Classifier Ensemble Blending F1:\",metrics.f1_score(y_test, y_stacked_test_pred))\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7peC__sbSITL"
      },
      "source": [
        "Model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D16-z6LnSMlu"
      },
      "source": [
        "'''\r\n",
        "test_result = model.evaluate(X_test, y_test)    # Default batch size is 32; https://www.tensorflow.org/api_docs/python/tf/keras/Model#evaluate\r\n",
        "print(test_result)\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}